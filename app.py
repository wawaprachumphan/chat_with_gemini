import streamlit as st
import pandas as pd
import google.generativeai as genai

# --- CONFIG Gemini ---
genai.configure(api_key=st.secrets.get("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY"))

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Gemini Model ‡πÅ‡∏ö‡∏ö multimodal ---
model = genai.GenerativeModel(
    model_name='gemini-1.5-flash',
    system_instruction='‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢'
)

# --- PAGE CONFIG ---
st.set_page_config(page_title="CSV Chatbot (Gemini)", layout="centered")
st.title("ü§ñ CSV Chatbot (Gemini)")

# --- UPLOAD ---
uploaded_file = st.file_uploader("üìÅ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .CSV", type=["csv"])

if uploaded_file:
    st.success("‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    df = pd.read_csv(uploaded_file)
    st.dataframe(df.head())

    # ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô bytes (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô part ‡∏Ç‡∏≠‡∏á Gemini multimodal)
    file_bytes = uploaded_file.read()
    uploaded_file.seek(0)  # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï pointer ‡∏Å‡∏•‡∏±‡∏ö (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pandas ‡∏≠‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô)

    # ‡∏£‡∏±‡∏ö prompt ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    user_prompt = st.chat_input("‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (chat history)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if user_prompt:
        # ‡πÅ‡∏™‡∏î‡∏á prompt ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        st.chat_message("user").markdown(user_prompt)
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Gemini multimodal content
        contents = [
            {
                "mime_type": "text/csv",
                "data": file_bytes
            },
            user_prompt
        ]

        try:
            response = model.generate_content(contents)
            reply = response.text
        except Exception as e:
            reply = f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
        st.chat_message("assistant").markdown(reply)
        st.session_state.chat_history.append({"role": "assistant", "content": reply})

else:
    st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .csv ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó")
